{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "CC-trade.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lsCn0GPJblu8",
        "1YdiHPJAChj2",
        "vvxOpjdCGdcI",
        "Qm4O2VVyGdcJ",
        "kan4ysrOGdcJ",
        "hQopiVMJGdcK",
        "ZUhNoio9GdcK",
        "sE0_iHviGdcK",
        "DlGhOkY-GdcL",
        "QJ0AocfgGdcL",
        "3F3ztZEXGdcL",
        "TTpvYMNPGdcL",
        "AltkyEGQGdcM",
        "0THS0UBSGdcM",
        "WW1mccsQGdcM",
        "WryzsQBlGdcM",
        "chltl6shGdcN",
        "7EL280JDGdcN",
        "IHzbqPY3GdcO",
        "DNcrERQ3GdcO",
        "Qg5wYdPCGdcO",
        "kepEGklIGdcO",
        "tBdDdireGdcP",
        "1YKHN7IvGdcP",
        "fVAm1UtwGdcP",
        "FIxUS415GdcP",
        "mRA9HHCbGdcQ",
        "ut2PtuVLGdcQ",
        "hOmUuD67GdcQ",
        "PwagVW0uGdcR",
        "mi8zHiofGdcR",
        "oI9jWM-gGdcR",
        "7BB3Sl-wGdcR",
        "F22oLqv7GdcS",
        "yHy7sfbqGdcS",
        "o87jkSVUGdcS",
        "A8TcuoqgGdcS",
        "YVLDuhCJGdcT",
        "T24Zy7uxGdcT",
        "DfMsHSx4GdcU",
        "oDafi0WTGdcU",
        "nvsO-GyrGdcU",
        "XzWPKlMiGdcV",
        "k9S5UidFGdcV",
        "mIJfGrs5GdcY",
        "gtNuBKxhGdcZ",
        "CzJjKSmWGdcZ",
        "kMNBVuX_GdcZ",
        "Vv8PBvKYGdcZ"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/futebolista99/PitagoraMaker/blob/master/CC_trade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "escL9uPRGdb7"
      },
      "source": [
        "# CCのLSTMによる予測"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsCn0GPJblu8"
      },
      "source": [
        "### 1.0 ライブラリのimport"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqS2JI7UIxHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92247e8c-d5f9-4164-c4b8-2462b4ef4c7d"
      },
      "source": [
        "!pip install japanize_matplotlib\n",
        "# !pip install janome\n",
        "# !pip uninstall torchtext\n",
        "# !pip install torchtext==0.8.1\n",
        "# !pip install transformers\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import japanize_matplotlib\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "import torch\n",
        "# import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "# from janome.tokenizer import Tokenizer\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# from torchtext.data import Field, LabelField, BucketIterator, TabularDataset\n",
        "from torchtext import datasets\n",
        "# from transformers import BertJapaneseTokenizer, BertModel\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, scale\n",
        "import pdb\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting japanize_matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from japanize_matplotlib) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->japanize_matplotlib) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->japanize_matplotlib) (1.15.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120274 sha256=281682a40b8d0ef9bbd8ab8cee575630bdd9eb60bbba4b8401c94c9da3192fa5\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/97/6b/e9e0cde099cc40f972b8dd23367308f7705ae06cd6d4714658\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-6abT71GdcB",
        "outputId": "8853eec2-b320-4326-baf0-b2b147eac724"
      },
      "source": [
        "# デバイスを取得\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: {0}\".format(DEVICE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqNqEJKT9Rak"
      },
      "source": [
        "### 1.1 実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96IjfZH8GdcD"
      },
      "source": [
        "\n",
        "`LSTM`は、3つのゲートと一つのセルで構成されます。\n",
        "\n",
        "それぞれを実装する式は次のようになります。($\\odot$は要素ごとの積)\n",
        "\n",
        "(1) 入力ゲート: $\\hspace{20mm}\\boldsymbol{i}_t = \\mathrm{\\sigma} \\left(\\boldsymbol{W}_i \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_i\\right)$\n",
        "\n",
        "(2) 忘却ゲート: $\\hspace{20mm}\\boldsymbol{f}_t = \\mathrm{\\sigma} \\left(\\boldsymbol{W}_f \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_f\\right)$  \n",
        "\n",
        "(3) 出力ゲート: $\\hspace{20mm}\\boldsymbol{o}_t = \\mathrm{\\sigma} \\left(\\boldsymbol{W}_o \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_o\\right)$  \n",
        "\n",
        "(4) セル:　　　 $\\hspace{20mm}\\boldsymbol{c}_t = \\boldsymbol{f}_t \\odot \\boldsymbol{c}_{t-1} + \\boldsymbol{i}_t \\odot \\tanh \\left(\\boldsymbol{W}_c \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_c\\right)$\n",
        "\n",
        "(5) 隠れ状態: 　$\\hspace{20mm}\\boldsymbol{h}_t = \\boldsymbol{o}_t \\odot \\tanh \\left(\\boldsymbol{c}_t \\right)$\n",
        "\n",
        "`RNN`では各ステップの関数の戻り値は隠れ状態のみ ($\\boldsymbol{h}_t$) でしたが、`LSTM`ではセル状態と隠れ状態の2つ ($\\boldsymbol{c}_t, \\boldsymbol{h}_t$) となるので注意してください。\n",
        "\n",
        "また、マスクに関してもセルと隠れ状態の両方に適用する必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoRHpdBYGdcE"
      },
      "source": [
        "class OurLSTM(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, attribute_dim):\n",
        "        super(OurLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        # self.padding_idx = padding_idx\n",
        "\n",
        "        self.attribute_dim = attribute_dim\n",
        "\n",
        "        # LSTMの各種パラメータ\n",
        "        # (1) 入力ゲート\n",
        "        self.linear_input_gate = nn.Linear(hidden_dim+attribute_dim, hidden_dim)\n",
        "        \n",
        "        # (2) 忘却ゲート\n",
        "        self.linear_forget_gate = nn.Linear(hidden_dim+attribute_dim, hidden_dim)\n",
        "        \n",
        "        # (3) 出力ゲート\n",
        "        self.linear_out_gate = nn.Linear(hidden_dim+attribute_dim, hidden_dim)\n",
        "        \n",
        "        # (4) セル\n",
        "        self.linear_cell = nn.Linear(hidden_dim+attribute_dim, hidden_dim)\n",
        "\n",
        "        # 最終ステップの隠れ層を入力として受け取る\n",
        "        self.linear_output = nn.Linear(hidden_dim, output_dim)\n",
        "        # self.act = nn.Sigmoid()\n",
        "        # self.act = nn.Softmax()\n",
        "\n",
        "        \n",
        "    def forward(self, inputs):  \n",
        "        # inputs = [max_len, batch_size, attribute_dim]\n",
        "\n",
        "        batch_size = inputs.shape[1]\n",
        "        \n",
        "        # セルと隠れ状態を初期化\n",
        "        hidden = self.init_hidden(batch_size)  # [batch_size, hidden_dim]\n",
        "        cell = self.init_cell(batch_size)  # [batch_size, hidden_dim]\n",
        "        hidden_prev = hidden  # [batch_size, hidden_dim]\n",
        "        cell_prev = cell  # [batch_size, hidden_dim]\n",
        "\n",
        "        # # padding対応用のマスクを作成\n",
        "        max_len = inputs.shape[0]\n",
        "        \n",
        "        for i in range(max_len):\n",
        "\n",
        "            # combined shape: [batch_size, attribute_dim+hidden_dim]\n",
        "            combined = torch.cat((inputs[i, :, :], hidden_prev), 1)\n",
        "\n",
        "            # (1) 入力ゲート\n",
        "            # i_t shape: [batch_size, hidden_dim]\n",
        "            i_t = torch.sigmoid(self.linear_input_gate(combined))\n",
        "            \n",
        "            # (2) 忘却ゲート\n",
        "            # f_t shape: [batch_size, hidden_dim]\n",
        "            f_t = torch.sigmoid(self.linear_forget_gate(combined)) \n",
        "            \n",
        "            # (3) 出力ゲート\n",
        "            # o_t shape: [batch_size, hidden_dim]\n",
        "            o_t = torch.sigmoid(self.linear_out_gate(combined)) \n",
        "            \n",
        "            # (4) セル\n",
        "            # c_t shape: [batch_size, hidden_dim]\n",
        "            c_t = f_t * cell_prev + i_t * torch.tanh(self.linear_cell(combined))\n",
        "            \n",
        "            # (5) 隠れ状態\n",
        "            # h_t shape: [batch_size, hidden_dim]\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "            \n",
        "            hidden_prev = h_t  # 隠れ状態の更新\n",
        "            cell_prev = c_t  # 隠れ状態の更新\n",
        "            \n",
        "        # 各文章の最終ステップの隠れ状態を入力 (パディングの一つ前の隠れ状態)\n",
        "        # output shape: [batch_size, output_dim]\n",
        "        output = self.linear_output(h_t)\n",
        "        # 出力をSigmoid関数で変換する\n",
        "        # output shape: [batch_size]\n",
        "\n",
        "        # 出力をSoftmax関数で変換する\n",
        "        # output = self.act(output.squeeze(1))\n",
        "        \n",
        "        return output\n",
        "\n",
        "    # 隠れ状態を初期化する関数\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(batch_size, self.hidden_dim, device=DEVICE)\n",
        "        return hidden\n",
        "\n",
        "    # Cellを初期化する関数\n",
        "    def init_cell(self, batch_size):\n",
        "        cell = torch.zeros(batch_size, self.hidden_dim, device=DEVICE)\n",
        "        return cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXrJoJ37GdcF"
      },
      "source": [
        "### 1.2 モデル学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsc5i4edGdcH"
      },
      "source": [
        "#### 1.2.1 モデル学習用関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdanZ5-1GdcH"
      },
      "source": [
        "def train_model(model, loss_function, optimizer, num_epochs=8):\n",
        "    # 学習モードに設定\n",
        "    model.train()\n",
        "\n",
        "    # モデルの学習\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = 0.0, 0.0\n",
        "        train_total = 0\n",
        "        for x, t in tqdm(train_loader):\n",
        "            x, t = x.to(DEVICE), t.unsqueeze(1).to(DEVICE)\n",
        "            train_total += x.shape[0]  # テストデータの大きさを取得\n",
        "\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            t = torch.transpose(t, 0, 1)\n",
        "            optimizer.zero_grad()  # 勾配の初期化          \n",
        "            t = t.to(torch.int64)\n",
        "\n",
        "            output = model(x)\n",
        "            loss = loss_function(output, t[0])  # 損失関数の計算\n",
        "            train_loss += loss.item()  # 損失の加算\n",
        "\n",
        "            train_acc += (output.argmax(dim=1) == t[0]).sum().item()  # 正答数の数え上げと可算\n",
        "            \n",
        "            loss.backward()  # 勾配の計算(逆伝播)\n",
        "            optimizer.step()  # パラメータの更新\n",
        "\n",
        "        avg_train_loss = train_loss / train_total  # 平均損失の計算\n",
        "        avg_train_acc = train_acc / train_total  # 正答率の計算\n",
        "\n",
        "        print(('Epoch [{}/{}], train_loss: {train_loss:.5f}, train_acc: {train_acc:.3f}')\n",
        "        .format(epoch+1, num_epochs, train_loss=avg_train_loss, train_acc=avg_train_acc))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvIaVihVGdcH"
      },
      "source": [
        "#### 1.2.2 モデル評価用関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7TzSNHqGdcH"
      },
      "source": [
        "def evaluation(model):\n",
        "    # テストデータの予測\n",
        "    model.eval()  # 推論モードに切替\n",
        "    with torch.no_grad():  # 計算グラフの構築をしないよう設定\n",
        "        # total = x_valid.shape[0]\n",
        "        test_acc = 0\n",
        "        valid_total = 0\n",
        "        for x, t in valid_loader:\n",
        "            x, t = x.to(DEVICE), t.unsqueeze(1).to(DEVICE)\n",
        "            valid_total += x.shape[0]  # テストデータの大きさを取得\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            t = torch.transpose(t, 0, 1)\n",
        "            t = t.to(torch.int64)\n",
        "            output = model(x)  # 予測の計算\n",
        "            \n",
        "            softmax = nn.Softmax(dim=1)\n",
        "            output = softmax(output)\n",
        "            test_acc += (output.argmax(dim=1) == t[0]).sum().item()  # 正解数の数え上げと加算\n",
        "            \n",
        "            pdb.set_trace()\n",
        "\n",
        "\n",
        "        print('Test Accuracy: {:.2f} %'.format(100 * test_acc / valid_total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xq3zMV5GdcG"
      },
      "source": [
        "#### 1.3 データセットとイテレータの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JshBkEnjACL5"
      },
      "source": [
        "DATA_PATH = '/content/data/'\n",
        "\n",
        "json_data = pd.read_json(DATA_PATH + 'BTC_USDT-1m-squeeze.json')\n",
        "json_data.to_csv(DATA_PATH + 'BTC_USDT-1m-squeeze.csv', encoding='utf-8')\n",
        "csv_data = pd.read_csv(DATA_PATH + 'BTC_USDT-1m-squeeze.csv')\n",
        "\n",
        "# json_data = pd.read_json(DATA_PATH + 'BTC_USDT-1m.json')\n",
        "# json_data.to_csv(DATA_PATH + 'BTC_USDT-1m.csv', encoding='utf-8')\n",
        "# csv_data = pd.read_csv(DATA_PATH + 'BTC_USDT-1m.csv')\n",
        "\n",
        "csv_data = csv_data.drop(csv_data.columns[0], axis=1)\n",
        "# cur_data = csv_data.iloc[:, [1, 5]].to_numpy()\n",
        "cur_data = csv_data.iloc[:, [1, 2, 3, 4, 5]].to_numpy()\n",
        "\n",
        "# DATA_PATH = '/content/data/'\n",
        "# json_data = pd.read_json(DATA_PATH + 'BTC_USDT-1m.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mycUlJXOZ0Se"
      },
      "source": [
        "# test_x_dim = 1000\n",
        "# test_y_dim = 1000\n",
        "# cur_data = np.empty((test_x_dim, test_y_dim))\n",
        "# test_x = 0\n",
        "# test_y = 0\n",
        "# test_z = 0\n",
        "# for i in range(test_x_dim):\n",
        "#   for j in range(test_y_dim):\n",
        "#     # test_z = test_x**7 - 2*test_x**6 + 3*test_x**5 - 4*test_x**4 + 5*test_x**3 - 6*test_x**2 + 7*test_x\n",
        "#     test_z = 10 * math.exp(-0.1*test_x) * math.cos(3*test_x) + 5 * math.exp(-0.2*test_y) * math.cos(2*test_y)\n",
        "#     test_z = 2 * math.sin(3*test_x) - 3*math.sin(2*test_y)\n",
        "#     cur_data[i][j] = test_z\n",
        "#     test_y += 0.01\n",
        "#   test_x += 0.01  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cur_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKD-5rgSh3Yk",
        "outputId": "9bf0be05-5d24-4382-c7ed-01ad76ef35d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.7068500e+04 4.7133200e+04 4.7058500e+04 4.7094020e+04 3.4425658e+01]\n",
            " [4.7094030e+04 4.7167660e+04 4.7092170e+04 4.7156650e+04 1.9483291e+01]\n",
            " [4.7156640e+04 4.7163300e+04 4.7070900e+04 4.7087060e+04 4.6914043e+01]\n",
            " ...\n",
            " [4.6339450e+04 4.6383000e+04 4.6336920e+04 4.6375170e+04 2.7507886e+01]\n",
            " [4.6375160e+04 4.6428210e+04 4.6350000e+04 4.6389870e+04 7.3221517e+01]\n",
            " [4.6389880e+04 4.6416140e+04 4.6354100e+04 4.6361790e+04 2.5874526e+01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fQND7UoqIGH"
      },
      "source": [
        "class TrainDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, x_train, t_train):\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.x_train)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x_train[idx], dtype=torch.float), \\\n",
        "            torch.tensor(self.t_train[idx], dtype=torch.float)\n",
        "\n",
        "price_length = 16\n",
        "attribute_dim = cur_data.shape[1]\n",
        "change_rate = 0.0003\n",
        "x_price_list = np.empty((cur_data.shape[0]-price_length, price_length, attribute_dim))\n",
        "t_price_list = np.empty((cur_data.shape[0]-price_length))\n",
        "\n",
        "for i in range(cur_data.shape[0]-price_length):\n",
        "  for j in range(price_length):\n",
        "    for k in range(attribute_dim):\n",
        "\n",
        "      x_price_list[i][j][k] = cur_data[i+j][k]\n",
        "\n",
        "  # pdb.set_trace()    \n",
        "  if cur_data[i+price_length][0] > (1+change_rate) * cur_data[i+price_length-1][0]:\n",
        "    t_price_list[i] = 2\n",
        "  elif cur_data[i+price_length][0] < (1-change_rate) * cur_data[i+price_length-1][0]:\n",
        "    t_price_list[i] = 0\n",
        "  else:\n",
        "    t_price_list[i] = 1\n",
        "\n",
        "# # 各行でデータを正規化\n",
        "# for i in range(x_price_list.shape[0]):\n",
        "#   x_price_list[i] = scale(x_price_list[i])\n",
        "\n",
        "# x_train, x_valid = \\\n",
        "#     train_test_split(x_price_list, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train, x_valid, t_train, t_valid = \\\n",
        "    train_test_split(x_price_list, t_price_list, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TrainDataset(x_train, t_train)\n",
        "valid_dataset = TrainDataset(x_valid, t_valid)\n",
        "\n",
        "# 1.DataLoaderの作成\n",
        "# 精度向上ポイント: バッチサイズの大小\n",
        "BATCH_SIZE = 8\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
        "                                           batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toEcel-FcS33",
        "outputId": "a2011493-00ee-4864-f9df-65e02649217a"
      },
      "source": [
        "print((t_train == 0).sum())\n",
        "print((t_train == 1).sum())\n",
        "print((t_train == 2).sum())\n",
        "\n",
        "print((t_train == 1).sum()/t_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "320\n",
            "318\n",
            "304\n",
            "[0.33757962]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hAmcC7IGdcI"
      },
      "source": [
        "#### 1.4 学習と評価を実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHJI3zz7GdcI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e52811fd-7cdf-43c7-ff61-23272e8e4bf3"
      },
      "source": [
        "# パラメータの設定\n",
        "\n",
        "# 隠れ層をxx次元とする。\n",
        "hidden_dim = 3\n",
        "\n",
        "# 出力層は1次元(ネガポジ判定のみのため)\n",
        "output_dim = 3\n",
        "\n",
        "# モデル定義\n",
        "our_lstm_model = OurLSTM(hidden_dim, output_dim, attribute_dim).to(DEVICE)\n",
        "loss_function = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(our_lstm_model.parameters(), lr=0.01)\n",
        "\n",
        "# モデルの学習（train_model関数を利用）\n",
        "our_lstm_model = train_model(our_lstm_model, loss_function, optimizer)\n",
        "\n",
        "# モデルの評価（evaluation関数を利用）\n",
        "evaluation(our_lstm_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 134.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/32], train_loss: 0.14144, train_acc: 0.326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 130.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/32], train_loss: 0.13790, train_acc: 0.312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 127.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/32], train_loss: 0.13801, train_acc: 0.321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 128.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/32], train_loss: 0.13782, train_acc: 0.332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 130.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/32], train_loss: 0.13792, train_acc: 0.322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 131.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/32], train_loss: 0.13785, train_acc: 0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 136.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/32], train_loss: 0.13785, train_acc: 0.325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 126.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/32], train_loss: 0.13797, train_acc: 0.343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 125.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/32], train_loss: 0.13813, train_acc: 0.297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 124.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/32], train_loss: 0.13792, train_acc: 0.334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 122.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/32], train_loss: 0.13782, train_acc: 0.313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 119.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/32], train_loss: 0.13806, train_acc: 0.330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 124.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/32], train_loss: 0.13789, train_acc: 0.318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 133.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/32], train_loss: 0.13776, train_acc: 0.317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 128.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/32], train_loss: 0.13793, train_acc: 0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 128.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/32], train_loss: 0.13788, train_acc: 0.327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 132.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/32], train_loss: 0.13781, train_acc: 0.356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 125.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/32], train_loss: 0.13814, train_acc: 0.332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 119.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/32], train_loss: 0.13788, train_acc: 0.328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 131.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/32], train_loss: 0.13785, train_acc: 0.328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 132.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/32], train_loss: 0.13824, train_acc: 0.311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 134.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/32], train_loss: 0.13793, train_acc: 0.328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 128.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/32], train_loss: 0.13801, train_acc: 0.327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 128.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/32], train_loss: 0.13775, train_acc: 0.341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 135.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/32], train_loss: 0.13790, train_acc: 0.314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 131.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/32], train_loss: 0.13786, train_acc: 0.330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 131.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/32], train_loss: 0.13785, train_acc: 0.342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 132.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/32], train_loss: 0.13791, train_acc: 0.325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 132.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/32], train_loss: 0.13801, train_acc: 0.317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 133.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/32], train_loss: 0.13820, train_acc: 0.329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 129.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/32], train_loss: 0.13827, train_acc: 0.317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:00<00:00, 129.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [32/32], train_loss: 0.13796, train_acc: 0.329\n",
            "> <ipython-input-56-d4d3fc7d484a>(8)evaluation()\n",
            "-> for x, t in valid_loader:\n",
            "(Pdb) p output\n",
            "tensor([[0.3363, 0.3256, 0.3380],\n",
            "        [0.3363, 0.3256, 0.3380],\n",
            "        [0.3363, 0.3256, 0.3380],\n",
            "        [0.3363, 0.3256, 0.3380],\n",
            "        [0.3363, 0.3256, 0.3380],\n",
            "        [0.3363, 0.3256, 0.3380],\n",
            "        [0.3363, 0.3256, 0.3380],\n",
            "        [0.3363, 0.3256, 0.3380]])\n",
            "(Pdb) q\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BdbQuit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-b810d7a28fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# モデルの評価（evaluation関数を利用）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mour_lstm_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-d4d3fc7d484a>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mvalid_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mvalid_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# テストデータの大きさを取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-d4d3fc7d484a>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mvalid_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mvalid_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# テストデータの大きさを取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBdbQuit\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YdiHPJAChj2"
      },
      "source": [
        "#### 補足 CrossEntropyLossの使い方"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnkxaEWK9QrH"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(2, 5, requires_grad=True)\n",
        "target = torch.empty(2, dtype=torch.long).random_(5)\n",
        "output = loss(input, target)\n",
        "\n",
        "# exp_input = torch.exp(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDdbYq8-9Vry",
        "outputId": "29389b56-7937-4d61-d940-6df56ebc611d"
      },
      "source": [
        "print(input)\n",
        "# print(exp_input)\n",
        "print(target)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.7185, -0.2972,  0.6030, -2.4340,  0.3861],\n",
            "        [ 0.3964, -1.0939,  2.6953,  0.3969, -0.3262]], requires_grad=True)\n",
            "tensor([2, 3])\n",
            "tensor(1.7329, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2k86gZsdg3y",
        "outputId": "ccc5d2ed-ab63-4bb7-fc8c-c61b3889e0f3"
      },
      "source": [
        "row1 = -1 * input[0, 2] + math.log(math.exp(input[0, 0])+math.exp(input[0, 1])+math.exp(input[0, 2])+math.exp(input[0, 3])+math.exp(input[0, 4]))\n",
        "row2 = -1 * input[1, 3] + math.log(math.exp(input[1, 0])+math.exp(input[1, 1])+math.exp(input[1, 2])+math.exp(input[1, 3])+math.exp(input[1, 4]))\n",
        "print(row1 + row2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.4658, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vV1WE6VCaQ7",
        "outputId": "8a9e3769-aee6-46ed-ab44-5ab312b18311"
      },
      "source": [
        "1.7329*2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.4658"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvxOpjdCGdcI"
      },
      "source": [
        "#### 1.2.3 予測結果の確認"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQdqzCqCGdcI"
      },
      "source": [
        "学習した`LSTM`のモデルを用いて、各文章が`positive`または`negative`のどちらに分類されたかを確認してみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tj9Bv3XGdcI"
      },
      "source": [
        "# ID化されたトークンを自然言語の文章に変換する関数\n",
        "def token2text(tokens, TEXT):\n",
        "    texts = []\n",
        "    for token in tokens:\n",
        "        text = TEXT.vocab.itos[token]\n",
        "        if text != \"<pad>\":\n",
        "            texts.append(text)\n",
        "        else:\n",
        "            break\n",
        "    return \"\".join(texts)\n",
        "\n",
        "\n",
        "# data_numで指定した数の例をテストデータから取り出し、出力結果を表示する関数\n",
        "def print_result(model, data_num=5):\n",
        "    # 出力例を確認\n",
        "    model.eval()  # 推論モードに切替\n",
        "    batch = next(iter(val_iter))  \n",
        "    predicts = model(batch.text)  # 予測の計算\n",
        "    predicts = torch.round(predicts)  # 予測値をラベルに変換\n",
        "    for i in range(data_num):\n",
        "        tokens = batch.text[0][:, i]\n",
        "        print(\"input text: {}\".format(token2text(tokens, TEXT)))\n",
        "        print(\"answer label: {}\".format(LABEL.vocab.itos[batch.label[i]]))\n",
        "        print(\"predicted label: {}\\n\".format(LABEL.vocab.itos[int(predicts[i])]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7eNVcsvGdcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0e9e2b-e1fd-479d-a5ad-45f13d14ccdc"
      },
      "source": [
        "# 出力例を確認（print_result関数を利用）\n",
        "print_result(our_lstm_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input text: ｆａ部門の連結売上高は、1,750億16百万円（前期比2.8％増)、全連結売上高に対する構成比は32.6％となりました\n",
            "answer label: positive\n",
            "predicted label: positive\n",
            "\n",
            "input text: 当社グループを取り巻くエネルギー業界においては、省エネルギー化や顧客ニーズの多様化などにより石油製品の需要が減少傾向にあるなか、石油<unk>の再編への動きが進むとともに、昨年４月に電力の<unk>が全面自由化された\n",
            "answer label: negative\n",
            "predicted label: negative\n",
            "\n",
            "input text: 経常損益につきましては、営業外収益に為替差益、営業外費用に支払利息を計上したこと等により、経常利益は12億71百万円（前年同期５億89百万円、115.6％増）となりました\n",
            "answer label: positive\n",
            "predicted label: positive\n",
            "\n",
            "input text: このような環境のもと、当事業年度における売上高は、前年比5.6％減の4,115,<unk>千円（うち輸出1,<unk>,<unk>千円　全売上高の25.8％）と２年連続の減収となりました\n",
            "answer label: negative\n",
            "predicted label: negative\n",
            "\n",
            "input text: 土地の<unk>売却が減少したこと等により減収となりましたが、分譲マンションにおいて高価格帯物件が増加したこと等により売上が増加、粗利益率も改善したことにより増益となりました\n",
            "answer label: positive\n",
            "predicted label: positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3eGe7NUGdcJ"
      },
      "source": [
        "### 1.3 より実践的なLSTMの実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZS1o7gNGdcJ"
      },
      "source": [
        "`LSTM`は`PyTorch`で用意されている`torch.nn.LSTM`を用いて実装することもできます。\n",
        "\n",
        "この実装方法では、モデルの学習時に各データを明示的に`loop`で回して実行する必要がないため、より簡単に実装することができます。\n",
        "\n",
        "そのため、実務では一般的にこちらの実装方法が用いられます。　参考までに、`toch.nn.LSTM`の実装例を示します。\n",
        "\n",
        "`torch.nn.LSTM`で主に使うパラメータは以下の5つです。他にもいくつか設定できるパラメータがあるので、必要に応じてドキュメントを確認してみてください。\n",
        "\n",
        "```\n",
        "input_size : 入力の次元数\n",
        "hidden_size : 隠れ状態の次元数\n",
        "num_layers : LSTMの層数\n",
        "dropout : 各LSTMの最終層のDropout Rate\n",
        "bidirectional : 双方向のLSTMを用いるかどうか (【発展】 Bidirectional LSTMの章で紹介します)\n",
        "```\n",
        "\n",
        "参考：https://pytorch.org/docs/master/generated/torch.nn.LSTM.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm4O2VVyGdcJ"
      },
      "source": [
        "#### 1.3.1 nn.LSTMを用いてモデルを定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F30hOEMZGdcJ"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = 1\n",
        "        \n",
        "        # 入力単語をEmbeddingする\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        \n",
        "        # LSTMを定義する\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=self.num_layers, bidirectional=False)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # inputs = (text, text_length)\n",
        "        inputs_text = inputs[0]  # 入力テキストを取得、 inputs_text = [sentence lengths, batch_size]\n",
        "        inputs_length = inputs[1]  # 入力テキストの文字列長を取得\n",
        "\n",
        "        # 文章内の各単語をEmbeddingする\n",
        "        # embedded shape: [sentence lengths, batch_size, embedding_dim]\n",
        "        embedded = self.embeddings(inputs_text)\n",
        "        # paddingされた単語で状態が更新されないようにする\n",
        "        packed_embedded = pack_padded_sequence(embedded, inputs_length, enforce_sorted=False) \n",
        "        \n",
        "        # LSTMに入力して各ステップで状態を更新\n",
        "        # output: 前ステップでの状態\n",
        "        # hidden: 最終ステップでの隠れ状態\n",
        "        # cell: 最終ステップでのセル\n",
        "        output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        \n",
        "        # LSTMの最終ステップの隠れ状態を入力する\n",
        "        # output shape: [batch_size, output_dim]\n",
        "        output = self.linear(hidden.squeeze(0))\n",
        "        # 出力をSigmoid関数で変換する\n",
        "        # output shape: [batch_size]\n",
        "        output = self.act(output.squeeze(1))\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kan4ysrOGdcJ"
      },
      "source": [
        "#### 1.3.2 モデルの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CswqaUltGdcK"
      },
      "source": [
        "# パラメータの設定\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 512\n",
        "hidden_dim = 256\n",
        "padding_idx = TEXT.vocab.stoi[\"<pad>\"]\n",
        "output_dim = 1\n",
        "\n",
        "# モデル定義\n",
        "lstm_model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx).to(DEVICE)\n",
        "loss_function = nn.BCELoss().to(DEVICE)\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# モデルの学習\n",
        "lstm_model = train_model(lstm_model, loss_function, optimizer)\n",
        "\n",
        "# モデルの評価\n",
        "evaluation(lstm_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQopiVMJGdcK"
      },
      "source": [
        "#### 1.3.3 予測結果の確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh4vtDYlGdcK"
      },
      "source": [
        "# 出力例を確認\n",
        "print_result(lstm_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZKBZCJZGdcK"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUhNoio9GdcK"
      },
      "source": [
        "## 【補足】Gradient Clipping（長系列への対処法）\n",
        "\n",
        "`LSTM`は長系列に対しても学習がうまく行きやすいモデルでしたが、一般の`RNN`における長系列の学習のTipsとして、**Gradient Clipping**に触れておきます。\n",
        "\n",
        "`RNN`では誤差逆伝播法が特に**Back Propagation Through Time (BPTT)**と呼ばれるものになり、各層のみならず各時点の勾配が乗算されます。\n",
        "\n",
        "そのため、通常よりも勾配が過大（或いは過小）になりやすいという特徴をもっています。\n",
        "\n",
        "こうした現象を**勾配爆発（消失）**と呼びますが、勾配爆発は学習を不安定化し収束を困難にします。\n",
        "\n",
        "![Clipping](/figures/Clipping.png)\n",
        "\n",
        "出典：Ian Goodfellow et. al, “Deep Learning”, MIT press, 2016 (http://www.deeplearningbook.org/)\n",
        "\n",
        "そこで、勾配の大きさを意図的に制限して対処しようというのが、`Gradient Clipping`と呼ばれる手法です。\n",
        "\n",
        "以下のように、明示的に`optimizer`から勾配を取得した後、`torch.nn.utils.clip_grad_norm`関数に通した上で勾配を適用することで実行できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgSw7qiIGdcK"
      },
      "source": [
        "# トレーニング時の各ステップでloss.backward()の後に以下を記述する。\n",
        "# 例としてモデルの定義名がlstmの場合を示す。\n",
        "\n",
        "clipping_value = 1  # 許容する勾配の最大値を設定する\n",
        "torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), clipping_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE0_iHviGdcK"
      },
      "source": [
        "### 【発展】 GRU (Gated Recurrent Unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt7Nu4obGdcL"
      },
      "source": [
        "`GRU (Gated Recurrent Unit)`は`LSTM`を単純化した構造を持ちます。そのため、モデルの表現力は`LSTM`と比較して劣りますが実行速度が速いと言った利点があります。\n",
        "\n",
        "`GRU`も`PyTorch`から`torch.nn.GRU`が用意されているため、簡単に実装することができます。\n",
        "\n",
        "`torch.nn.GRU`で主に使うパラメータは以下の5つです。他にもいくつか設定できるパラメータがあるので、必要に応じてドキュメントを確認してみてください。\n",
        "\n",
        "```\n",
        "input_size : 入力の次元数\n",
        "hidden_size : 隠れ状態の次元数\n",
        "num_layers : LSTMの層数\n",
        "dropout : 各LSTMの最終層のDropout Rate\n",
        "bidirectional : 双方向のGRUを用いるかどうか\n",
        "```\n",
        "\n",
        "参考 : https://pytorch.org/docs/master/generated/torch.nn.GRU.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlGhOkY-GdcL"
      },
      "source": [
        "#### nn.GRUを用いてモデルを定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM5_AG-4GdcL"
      },
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = 1\n",
        "        \n",
        "        # 入力単語をEmbeddingする\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        \n",
        "        # GRUを定義する\n",
        "        # nn.GRU以外はLSTMの時と同様で利用できる\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=self.num_layers, bidirectional=False)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # inputs = (text, text_length)\n",
        "        inputs_text = inputs[0]  # 入力テキストを取得、 inputs_text = [sentence lengths, batch_size]\n",
        "        inputs_length = inputs[1]  # 入力テキストの文字列長を取得\n",
        "\n",
        "        # 文章内の各単語をEmbeddingする\n",
        "        # embedded shape: [sentence lengths, batch_size, embedding_dim]\n",
        "        embedded =  # WRITE ME\n",
        "        # paddingされた単語で状態が更新されないようにする\n",
        "        packed_embedded =  # WRITE ME\n",
        "        \n",
        "        # GRUに入力して各ステップで状態を更新\n",
        "        # output: 前ステップでの状態\n",
        "        # hidden: 最終ステップでの隠れ状態\n",
        "        output, hidden =  # WRITE ME\n",
        "        \n",
        "        # GRUの最終ステップの隠れ状態を入力する\n",
        "        # output shape: [batch_size, output_dim]\n",
        "        output =  # WRITE ME\n",
        "        # 出力をSigmoid関数で変換する\n",
        "        # output shape: [batch_size]\n",
        "        output =  # WRITE ME\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ0AocfgGdcL"
      },
      "source": [
        "#### モデルの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWW_qXP0GdcL"
      },
      "source": [
        "# パラメータの設定\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 512\n",
        "hidden_dim = 256\n",
        "padding_idx = TEXT.vocab.stoi[\"<pad>\"]\n",
        "output_dim = 1\n",
        "\n",
        "# モデル定義\n",
        "gru_model = GRU(vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx).to(DEVICE)\n",
        "loss_function = nn.BCELoss().to(DEVICE)\n",
        "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "\n",
        "# モデルの学習\n",
        "gru_model = train_model(gru_model, loss_function, optimizer)\n",
        "\n",
        "# モデルの評価\n",
        "evaluation(gru_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F3ztZEXGdcL"
      },
      "source": [
        "#### 予測結果の確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v8F-kzyGdcL"
      },
      "source": [
        "# 出力例を確認\n",
        "print_result(gru_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTpvYMNPGdcL"
      },
      "source": [
        "### 【発展】 Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJZmXkopGdcM"
      },
      "source": [
        "`LSTM`を定義する時に、引数`bidirectional`の値を`True`に設定することで`Bidirectional LSTM` (双方向LSTM)を用いることができます。\n",
        "\n",
        "`Bidirectional LSTM`では、過去から未来方向へのタイムステップだけでなく、未来から過去方向への学習も行います。そのため、通常の`LSTM`より文脈をうまく捉えることができベンチマークなどで高いスコアが出ています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AltkyEGQGdcM"
      },
      "source": [
        "#### Bi-LSTMの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aAbg7ozGdcM"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = 1\n",
        "\n",
        "        # 入力単語をEmbeddingする\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # LSTMを定義する\n",
        "        # bidirectionalの値をTrueに設定することでBidirectional LSTMを用いることができる\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=self.num_layers, bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_dim*2, output_dim)  # 2方向になるので隠れ状態も2倍になる\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # inputs = (text, text_length)\n",
        "        inputs_text = inputs[0]  # 入力テキストを取得、 inputs_text = [sentence lengths, batch_size]\n",
        "        inputs_length = inputs[1]  # 入力テキストの文字列長を取得\n",
        "\n",
        "        # 文章内の各単語をEmbeddingする\n",
        "        # embedded shape: [sentence lengths, batch_size, embedding_dim]\n",
        "        embedded =  # WRITE ME\n",
        "        # paddingされた単語で状態が更新されないようにする\n",
        "        packed_embedded =  # WRITE ME\n",
        "        \n",
        "        # Bi-LSTMに入力して各ステップで状態を更新\n",
        "        # output: 前ステップでの状態\n",
        "        # hidden: 最終ステップでの隠れ状態\n",
        "        # cell: 最終ステップでのセル\n",
        "        output, (hidden, cell) =  # WRITE ME\n",
        "        \n",
        "        # LSTMの最終ステップの隠れ状態を入力する\n",
        "        # 2方向LSTMの最終ステップの各隠れ状態連結して入力する\n",
        "        # output shape: [batch_size, output_dim]\n",
        "        output =  # WRITE ME\n",
        "        # 出力をSigmoid関数で変換する\n",
        "        # output shape: [batch_size]\n",
        "        output =  # WRITE ME\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0THS0UBSGdcM"
      },
      "source": [
        "#### モデルの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY_lJ6y3GdcM"
      },
      "source": [
        "# パラメータの設定\n",
        "vocab_size =  # WRITE ME\n",
        "embedding_dim =  # WRITE ME\n",
        "hidden_dim =  # WRITE ME\n",
        "padding_idx =  # WRITE ME\n",
        "output_dim =  # WRITE ME\n",
        "\n",
        "# モデル定義\n",
        "bi_lstm_model =  # WRITE ME\n",
        "loss_function =  # WRITE ME\n",
        "optimizer =  # WRITE ME\n",
        "\n",
        "# モデルの学習（train_model関数を利用）\n",
        "bi_lstm_model =  # WRITE ME\n",
        "\n",
        "# モデルの評価（evaluation関数を利用）\n",
        "# WRITE ME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW1mccsQGdcM"
      },
      "source": [
        "#### 予測結果の確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9ij20fpGdcM"
      },
      "source": [
        "# 出力例を確認（print_result関数を利用）\n",
        "# WRITE ME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6mCaFnNGdcM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WryzsQBlGdcM"
      },
      "source": [
        "## 課題2. Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chltl6shGdcN"
      },
      "source": [
        "### 1. データセットの読み込みと単語・品詞のID化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFP4dDgaGdcN"
      },
      "source": [
        "今回はデータセットとして、英文とその日本語対訳がセットになった Tanaka Corpus ( http://www.edrdg.org/wiki/index.php/Tanaka_Corpus ) を使用します。\n",
        "\n",
        "（厳密にはデータサイズの関係で、Tanaka Corpusの一部を抽出した https://github.com/odashi/small_parallel_enja を使っています。）\n",
        "\n",
        "train.enとtrain.jaの中身は次のようになっています。\n",
        "\n",
        "- train.enの中身 (英語の文)\n",
        "```\n",
        "i can 't tell who will arrive first .\n",
        "many animals have been destroyed by men .\n",
        "i 'm in the tennis club .\n",
        "︙\n",
        "```\n",
        "\n",
        "- train.jaの中身(日本語の文、対訳)\n",
        "```\n",
        "誰 が 一番 に 着 く か 私 に は 分か り ま せ ん 。\n",
        "多く の 動物 が 人間 に よ っ て 滅ぼ さ れ た 。\n",
        "私 は テニス 部員 で す 。\n",
        "︙\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg6S07ikGdcN"
      },
      "source": [
        "この通り、そのまま読み込むと単語のまま読み込まれます。\n",
        "\n",
        "これでは計算的には扱いづらいので、第7章の時と同様にそれぞれの単語を**数字によるIDに置き換え**ましょう。\n",
        "\n",
        "この数値化には、`janome.tokenizer`の`Tokenizer`を用います。 `index=0`は、未知語を表す`<unk>`に割り当てられ、\n",
        "\n",
        "`index=1`はpaddingを表す`<pad>`に割り当てられます。\n",
        "\n",
        "ここではTokenizerの仕様については深く立ち入りませんので、気になる方は公式ドキュメントをチェックしてください。\n",
        "\n",
        "なお、読み込む際には\n",
        "\n",
        "- 文頭を表す仮想単語（**SOS**, Start Of Sentence）として`<sos>`\n",
        "\n",
        "- 文末を表す仮想単語（**EOS**, End Of Sentence）として`<eos>`\n",
        "    \n",
        "を付加します。\n",
        "\n",
        "参考： https://mocobeta.github.io/janome/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "516mVSpJGdcN"
      },
      "source": [
        "# 日本語用のtokenizer\n",
        "ja_t = Tokenizer()\n",
        "def tokenizer_ja(text): \n",
        "    return  # WRITE ME\n",
        "\n",
        "# 英語用のtokenizer\n",
        "spacy_en = spacy.load('en')\n",
        "def tokenizer_en(text):\n",
        "    return  # WRITE ME (spacy_en.tokenizer()を利用)\n",
        "\n",
        "# 各種Fieldを定義\n",
        "# 文章の始まりを表す<sos>と文章の終わりを表す<eos>を各文章に付与する\n",
        "SOURCE = Field(sequential=True, tokenize=tokenizer_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, include_lengths=True)\n",
        "TARGET = Field(sequential=True, tokenize=tokenizer_ja, init_token=\"<sos>\", eos_token=\"<eos>\", lower=False, include_lengths=True)\n",
        "\n",
        "train, val = TabularDataset.splits(\n",
        "    path=\"/root/userspace/public/day5/chap10/data/\",\n",
        "    train=\"train.csv\", validation=\"val.csv\",\n",
        "    format=\"csv\", skip_header=True,\n",
        "    fields=[(\"ids\", None), (\"source\", SOURCE), (\"target\", TARGET)]\n",
        ")\n",
        "\n",
        "\n",
        "# SOURCEとTARGETそれぞれの単語に番号を振る\n",
        "# 最低出現回数をmin_freqで指定\n",
        "# WRITE ME (SOURCE側)\n",
        "# WRITE ME (TARGET側)\n",
        "\n",
        "# イテレータの作成\n",
        "batch_size = 128\n",
        "train_iter, val_iter = BucketIterator.splits(\n",
        "    (train, val), batch_size=batch_size, device=DEVICE, sort=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EL280JDGdcN"
      },
      "source": [
        "### 2. 各層クラスの実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMAvMjvnGdcN"
      },
      "source": [
        "`Encoder-Decoder`モデルでは、入力系列 (`Source`)を`Encoder`を用いて固定長のベクトルに変換し、得られたベクトルを`Decoder`に入力し出力系列 (`Target`)に近くなるように系列を生成します。\n",
        "\n",
        "また、生成を行う際には1ステップずつ逐次的に`LSTM`を実行したいので、状態を保持する機能も持たせる必要があります。\n",
        "\n",
        "はじめに、`Encoder`と`Decoder`をそれぞれ定義します。　その後、それらを組み合わせることで翻訳を行う`Seq2Seq`モデルを定義します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJiCZkEWGdcN"
      },
      "source": [
        "# Encoder側の定義\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size, n_layers, dropout, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        self.padding_idx = padding_idx\n",
        "        \n",
        "        self.embed =  # WRITE ME\n",
        "        \n",
        "        # 今回はtorch.nn.LSTMを用いて実装してください\n",
        "        self.lstm =  # WRITE ME\n",
        "\n",
        "\n",
        "    def forward(self, src_text, src_length):\n",
        "        embedded = self.embed(src_text)\n",
        "        # paddingされた単語で状態が更新されないようにする\n",
        "        packed_embedded = pack_padded_sequence(embedded, src_length, enforce_sorted=False)\n",
        "        \n",
        "        # embeddingの結果をLSTMに入力\n",
        "        outputs, (hidden, cell) =  # WRITE ME\n",
        "        outputs, sent_len = pad_packed_sequence(outputs)\n",
        "        \n",
        "        return outputs, hidden, cell, sent_len\n",
        "\n",
        "    \n",
        "# Decoder側の定義\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, n_layers, output_size, dropout, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        self.embed =  # WRITE ME\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # 今回はtorch.nn.LSTMを用いて実装してください\n",
        "        self.lstm =  # WRITE ME\n",
        "        \n",
        "        # 線形層を用いて、hidden_size -> output_sizeの空間に射影してください\n",
        "        self.fc_out =  # WRITE ME\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        embedded = self.embed(input).unsqueeze(0) \n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # embeddingの結果をLSTMに入力\n",
        "        _, (hidden, cell) =  # WRITE ME\n",
        "        prediction = self.fc_out(hidden.squeeze(0))\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63TaBnlqGdcO"
      },
      "source": [
        "今回は以下のようなモデルを実装してみましょう。（ここでは、図中の`hidden layer 2`は省略します。）\n",
        "\n",
        "<img src=\"./figures/seq2seq.png\" width=\"50%\">\n",
        "\n",
        "引用：https://github.com/tensorflow/nmt\n",
        "\n",
        "モデルの構造としては、\n",
        "\n",
        "- 翻訳したい文をRNNで状態ベクトルによる表現に変換する（**Encoder**）\n",
        "- 上述の状態ベクトルを考慮しつつ、**訳文の1単語先を予測**する（**Decoder**）\n",
        "\n",
        "という大きく2つの部分からなっています。\n",
        "\n",
        "学習タスクとしては、訳文の時点$t$の単語から時点$t+1$の単語の予測タスクとなっていますが、翻訳前の文章も考慮するわけです。\n",
        "\n",
        "なお、誤差関数は多クラス交差エントロピーで、具体的には次のようになります。（ミニバッチサイズ：$N$、系列長：$T$、辞書サイズ：$K$）\n",
        "\n",
        "$$\n",
        "    \\mathrm{E}\\left(\\{\\boldsymbol{S}^{(n)}\\}_{n=1,\\ldots,N},\\{\\boldsymbol{Y}^{(n)}\\}_{n=1,\\ldots,N}\\right) = -\\frac{1}{N}\\sum^N_{n=1}\\sum^T_{t=1}\\sum^K_{k=1} s^{(n)}_{t, k} \\log y^{(n)}_{t, k}\n",
        "$$\n",
        "\n",
        "$$\\left(\\boldsymbol{S}^{(n)} = [\\boldsymbol{s}^{(n)}_1\\ \\ \\boldsymbol{s}^{(n)}_2\\ \\ \\cdots \\ \\ \\boldsymbol{s}^{(n)}_T]\\in\\{0,1\\}^{K \\times T},\\ \\ \\boldsymbol{Y}^{(n)} = [\\boldsymbol{y}^{(n)}_1\\ \\ \\boldsymbol{y}^{(n)}_2\\ \\ \\cdots \\ \\ \\boldsymbol{y}^{(n)}_T]\\in\\mathbb{R}^{K \\times T}\\ \\ \\ n=1,2,\\ldots,N\\right)$$\n",
        "\n",
        "$\\boldsymbol{s}^{(n)}_t$は$n$番目の系列の時点$t$での単語のone_hot表現です。\n",
        "\n",
        "また、各$\\boldsymbol{y}^{(n)}_t$はモデルからの出力なので、モデルのパラメータ$\\boldsymbol{\\theta}$に依存しており, 最適化はこの$\\boldsymbol{\\theta}$について行われます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-H_KYnVGdcO"
      },
      "source": [
        "# 定義したEncoderとDecoderを用いてSeq2Seqモデルを定義する\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_text = src[0]  # Sourceのテキストを取得\n",
        "        src_length = src[1]  # Sourceのテキストを取得\n",
        "        trg_text = trg[0]  # Targetのテキストを取得\n",
        "        trg_length = trg[1]  # Targetのテキスト長を取得\n",
        "        batch_size = trg_text.shape[1]  # Batch Sizeを取得\n",
        "        max_trg_len = trg_text.shape[0]  # Targetの最大文字列長を取得\n",
        "        trg_vocab_size = self.decoder.output_size  # TargetのVocaburay数を取得\n",
        "\n",
        "        # Decoderの出力結果を保持する配列\n",
        "        outputs = torch.zeros(max_trg_len, batch_size, trg_vocab_size).to(DEVICE)\n",
        "\n",
        "        # Encoderの最後の隠れ状態をDecoderの初期値とする\n",
        "        _, encoder_hidden, encoder_cell, _ = self.encoder(src_text, src_length)\n",
        "\n",
        "        # 文章の始まりを表す<sos>トークンをDecoderに入力する\n",
        "        input = trg_text[0, :]\n",
        "\n",
        "        # 各ステップを実行\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "        for step in range(1, max_trg_len):\n",
        "            output, decoder_hidden, decoder_cell =  # WRITE ME\n",
        "            outputs[step] = output\n",
        "\n",
        "            # 予測結果の配列から、もっとも確率が高い予測トークンを取得する\n",
        "            # output = [0.1, 0.1, 0.05, ...., 0.7, 0.05]\n",
        "            top1 =  # WRITE ME\n",
        "\n",
        "            # 次のstepでの入力を更新\n",
        "            input = top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHzbqPY3GdcO"
      },
      "source": [
        "### 3. 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNcrERQ3GdcO"
      },
      "source": [
        "#### モデル学習用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nanVCxDlGdcO"
      },
      "source": [
        "def train_seq2seq_model(model, loss_function, optimizer, num_epochs=10, clip=1.0):\n",
        "    # 学習モードに設定\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        for batch in tqdm(train_iter):\n",
        "            optimizer.zero_grad()  # 勾配の初期化\n",
        "            output = model(batch.source, batch.target)  # 予測の計算（順伝播）\n",
        "            \n",
        "            # 予測結果の整形と正解データの抽出\n",
        "            output_size = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_size)  # <sos>を除く\n",
        "            \n",
        "            trg_text = batch.target[0]\n",
        "            trg = trg_text[1:].view(-1)  # <sos>を除く\n",
        "            \n",
        "            loss = loss_function(output, trg)  # 損失関数の計算\n",
        "            train_loss += loss.item()  # 損失の加算\n",
        "            loss.backward()  # 勾配の計算(逆伝播)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Gradient Clipping\n",
        "            optimizer.step()  # パラメータの更新\n",
        "            \n",
        "        avg_train_loss = train_loss / len(train)  # 平均損失の計算\n",
        "        print(('Epoch [{}/{}]: train_loss: {train_loss:.5f}')\n",
        "          .format(epoch+1, num_epochs, train_loss=avg_train_loss))\n",
        "        \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg5wYdPCGdcO"
      },
      "source": [
        "#### モデル評価用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVpATEEfGdcO"
      },
      "source": [
        "def evaluate_seq2seq_model(model, loss_function):\n",
        "    model.eval()  # 推論モードに切替\n",
        "    with torch.no_grad():  # 計算グラフの構築をしないよう設定\n",
        "        test_loss = 0\n",
        "        for batch in tqdm(val_iter):\n",
        "            output = model(batch.source, batch.target)  # 予測の計算（順伝播）\n",
        "\n",
        "            # 予測結果の整形と正解データの抽出\n",
        "            output_size = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_size)  # <sos>を除く\n",
        "            \n",
        "            trg_text = batch.target[0]\n",
        "            trg = trg_text[1:].view(-1)  # <sos>を除く\n",
        "\n",
        "            loss = loss_function(output, trg)  # 損失関数の計算\n",
        "            test_loss += loss.item()\n",
        "\n",
        "        avg_test_loss = test_loss / len(val)\n",
        "        print('Test Loss: {test_loss:.5f}'.format(test_loss=avg_test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kepEGklIGdcO"
      },
      "source": [
        "#### 学習と評価を実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG76kmqxGdcP"
      },
      "source": [
        "# ハイパーパラメータの設定\n",
        "input_size = len(SOURCE.vocab)\n",
        "output_size = len(TARGET.vocab)\n",
        "encoder_embed_size = 256\n",
        "decoder_embed_size = 256\n",
        "hidden_size = 512\n",
        "n_layers = 1\n",
        "encoder_dropout = 0.5\n",
        "decoder_dropout = 0.5\n",
        "src_padding_idx = SOURCE.vocab.stoi[\"<pad>\"]\n",
        "trg_padding_idx = TARGET.vocab.stoi[\"<pad>\"]\n",
        "\n",
        "# EncoderクラスとDecoderクラスを生成\n",
        "encoder = Encoder(input_size, encoder_embed_size, hidden_size, n_layers, encoder_dropout, src_padding_idx)\n",
        "decoder = Decoder(decoder_embed_size, hidden_size, n_layers, output_size, decoder_dropout, trg_padding_idx)\n",
        "\n",
        "# デバイスを指定して翻訳モデルを生成\n",
        "seq2seq_model = Seq2Seq(encoder, decoder).to(DEVICE)\n",
        "optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=trg_padding_idx)\n",
        "\n",
        "# モデルのパラメータを初期化\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "seq2seq_model.apply(init_weights)\n",
        "\n",
        "# モデルの学習（train_seq2seq_model関数を利用）\n",
        "seq2seq_model =  # WRITE ME\n",
        "\n",
        "# モデルの評価（evaluate_seq2seq_model関数を利用）\n",
        "# WRITE ME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBdDdireGdcP"
      },
      "source": [
        "### 4. 生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH00a5moGdcP"
      },
      "source": [
        "トレーニングデータで学習した`Seq2Seq`モデルを用いて、テストデータの出力結果を確認してみます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YKHN7IvGdcP"
      },
      "source": [
        "#### 正解の文章と生成された文章を確認する関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ir966GrGdcP"
      },
      "source": [
        "# ID変換された各単語のリストを文章に変換する\n",
        "def token2text(tokens, VOCAB):\n",
        "    texts = []\n",
        "    for token in tokens[1:]:  # <sos>を除く\n",
        "        text = VOCAB.vocab.itos[token]\n",
        "        if text != \"<eos>\":\n",
        "            texts.append(text)\n",
        "        else:\n",
        "            break\n",
        "       \n",
        "    return \" \".join(texts)\n",
        "\n",
        "# 出力例を確認\n",
        "def print_seq2seq_result(model):\n",
        "    model.eval()  # 推論モードに切替\n",
        "    batch = next(iter(val_iter))\n",
        "    predicts = model(batch.source, batch.target)  # 予測の計算\n",
        "    predicts = predicts.argmax(2)  # 予測値のみを取得: (N, T, V) =>  (N, T)\n",
        "    for i in range(5):\n",
        "        source = batch.source[0][:, i]\n",
        "        target = batch.target[0][:, i]\n",
        "        print(\"English (input): {0}\".format(token2text(source, SOURCE)))\n",
        "        print(\"Japanese (answer): {0}\".format(token2text(target, TARGET)))\n",
        "        print(\"predicted: {0}\\n\".format(token2text(predicts[:, i], TARGET)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVAm1UtwGdcP"
      },
      "source": [
        "#### 生成された文章の確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g3G_35yGdcP"
      },
      "source": [
        "# 出力例を確認（print_seq2seq_result関数を利用）\n",
        "# WRITE ME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIxUS415GdcP"
      },
      "source": [
        "### 【補足】Teacher Forcing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPH6Qs3eGdcQ"
      },
      "source": [
        "`Teacher Forcing`とは、モデルの学習時に`Decoder`の入力として1ステップ前の`Decoder`の出力結果を用いるのではなく、\n",
        "\n",
        "正解データのターゲット配列をそのまま用いる手法です。　\n",
        "\n",
        "このようにすることで、モデルの学習が安定し収束が早くなるというメリットがあります。\n",
        "\n",
        "先ほど実装した`Seq2Seq`クラスに3点変更を加えることで、`teacher forcing`を用いた日英翻訳モデルを作成することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OU2DAE8GdcQ"
      },
      "source": [
        "# Teacher Forcingを用いたSeq2Seqモデルを定義する\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    # [変更箇所 1] 引数にteacher forcingを用いる比率を追加する\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        src_text = src[0]  # Sourceのテキストを取得\n",
        "        src_length = src[1]  # Sourceのテキストを取得\n",
        "        \n",
        "        trg_text = trg[0]  # Targetのテキストを取得\n",
        "        trg_length = trg[1]  # Targetのテキスト長を取得\n",
        "        \n",
        "        batch_size = trg_text.shape[1]  # Batch Sizeを取得\n",
        "        \n",
        "        max_trg_len = trg_text.shape[0]  # Targetの最大文字列長を取得\n",
        "        trg_vocab_size = self.decoder.output_size  # TargetのVocaburay数を取得\n",
        "\n",
        "        # Decoderの出力結果を保持する配列\n",
        "        outputs = torch.zeros(max_trg_len, batch_size, trg_vocab_size).to(DEVICE)\n",
        "\n",
        "        # Encoderの最後の隠れ状態をDecoderの初期値とする\n",
        "        _, encoder_hidden, encoder_cell, _ = self.encoder(src_text, src_length)\n",
        "\n",
        "        # 文章の始まりを表す<sos>トークンをDecoderに入力する\n",
        "        input = trg_text[0, :]\n",
        "\n",
        "        # 各ステップを実行\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "        for step in range(1, max_trg_len):\n",
        "            output, decoder_hidden, decoder_cell = self.decoder(input, decoder_hidden, decoder_cell)\n",
        "            outputs[step] = output\n",
        "            \n",
        "            # [変更箇所 2] 現在のステップでteacher forcingを用いるかどうか\n",
        "            teacher_force =  # WRITE ME (random.random()を利用)\n",
        "\n",
        "            # 予測結果の配列から、もっとも確率が高い予測トークンを取得する\n",
        "            # output = [0.1, 0.1, 0.05, ...., 0.7, 0.05]\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # [変更箇所 3] techer forceingを用いる場合 (teacher_foce=True)は、\n",
        "            # target配列から該当する値を抽出し、用いない場合 (teacher_foce=False)は\n",
        "            # 予測結果 (top1)を次のステップでの入力とする\n",
        "            input =  # WRITE ME\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2eFUlziGdcQ"
      },
      "source": [
        "上記で学習に用いていた`Seq2Seq`の代わりに、新たに作成した`Teach Forcing`を用いた`Seq2Seq`のモデルを指定することで、コードを変更することなく動作確認を行うことができます。\n",
        "\n",
        "余力がある場合は、`Teach Forcing`を用いた場合とそうでない場合で収束にかかる時間やモデルの精度を比較してみてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54KnEq9zGdcQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRA9HHCbGdcQ"
      },
      "source": [
        "## 課題3. Attention Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut2PtuVLGdcQ"
      },
      "source": [
        "### 1. Attention Modelの概要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xNCO4C8GdcQ"
      },
      "source": [
        "今回は Luong et al., 2015のGlobal attentionモデルを実装します。\n",
        "\n",
        "- \"Effective Approaches to Attention-based Neural Machine Translation\", Minh-Thang Luong et al., EMNLP 2015 https://arxiv.org/abs/1508.04025\n",
        "\n",
        "課題2で実装したモデルは左図、ここで実装するモデルは右図になります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3lTP-6yGdcQ"
      },
      "source": [
        "<img src=\"./figures/attention-1.png\" width=\"1000mm\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOmUuD67GdcQ"
      },
      "source": [
        "#### Decoderの各ステップにおける計算の手順\n",
        "\n",
        "`Encoder`の各ステップの隠れ層を\n",
        "\n",
        "$$\n",
        "    \\boldsymbol{\\bar{h}} = \\{\\boldsymbol{\\bar{h}}_1, \\boldsymbol{\\bar{h}}_2, \\ldots, \\boldsymbol{\\bar{h}}_s, \\ldots, \\boldsymbol{\\bar{h}}_S\\}\n",
        "$$\n",
        "\n",
        "`Decoder`の各ステップの隠れ層を\n",
        "\n",
        "$$\n",
        "    \\boldsymbol{h} = \\{\\boldsymbol{h}_1, \\boldsymbol{h}_2, \\ldots, \\boldsymbol{h}_t, \\ldots, \\boldsymbol{h}_T\\}\n",
        "$$\n",
        "\n",
        "とします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73NRCbgKGdcR"
      },
      "source": [
        "`Attention Layer`の計算手順は以下のようになります。\n",
        "\n",
        "1. Decoderの各時点の出力に対して、入力系列のどのステップに注目するかを表すscore関数の値を計算します。\n",
        "$$\n",
        "   \\mathrm{score}\\left(\\boldsymbol{\\bar{h}}_s,\\ \\boldsymbol{h}_t\\right) = \\boldsymbol{h}_t^{\\mathrm{T}} \\boldsymbol{W}_a \\boldsymbol{\\bar{h}}_s\n",
        "$$\n",
        "なお、score関数には任意性があり、今回用いる関数以外にも例えば以下の様なものが提案されています。\n",
        "$$\n",
        "\\mathrm{score}\\left(\\boldsymbol{\\bar{h}}_s,\\ \\boldsymbol{h}_t\\right) =\n",
        "\\begin{cases}\n",
        "    {\\boldsymbol{h}_t}^{\\mathrm{T}} \\boldsymbol{\\bar{h}}_s \\\\\n",
        "    \\boldsymbol{v}^{\\mathrm{T}} \\tanh\\left(\\boldsymbol{W}_{ad} \\boldsymbol{h}_t + \\boldsymbol{W}_{ae} \\boldsymbol{\\bar{h}}_s\\right)\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "2. 次にscoreをsoftmax関数により**重み**$\\ \\boldsymbol{a}_t(s)$に変換します。\n",
        "$$\n",
        "    \\boldsymbol{a}_t(s) = \\frac{\\exp\\left[\\mathrm{score}\\left(\\boldsymbol{\\bar{h}}_s,\\ \\boldsymbol{h}_t\\right)\\right]}{\\sum^S_{s'=1}\\exp\\left[\\mathrm{score}\\left(\\boldsymbol{\\bar{h}}_s,\\ \\boldsymbol{h}_t\\right)\\right]}\n",
        "$$\n",
        "3. 2.で計算した重みを元に、Encoderの出力の加重平均ベクトル (**文脈ベクトル**) $\\boldsymbol{c}_t$ を計算します。\n",
        "$$\n",
        "    \\boldsymbol{c}_t = \\sum^S_{s=1} \\boldsymbol{a}_t(s) \\boldsymbol{\\bar{h}}_s\n",
        "$$\n",
        "4. 3.で計算した文脈ベクトル$\\boldsymbol{c}_t$と元の出力$\\boldsymbol{h}_t$から、新しい出力ベクトル$\\boldsymbol{\\tilde{h}}_t$を計算します。\n",
        "$$\n",
        "    \\boldsymbol{\\tilde{h}}_t = \\tanh\\left(\\boldsymbol{W}_c \\left[\\begin{array}{c} \\boldsymbol{c}_t \\\\ \\boldsymbol{h}_t \\end{array}\\right] + \\boldsymbol{b}\\right)\n",
        "$$\n",
        "\n",
        "論文では$\\boldsymbol{\\tilde{h}}_t$が次のタイムステップのLSTMにfeedされる方法も示されており、他の論文でもそのようにしているものも多いです。\n",
        "\n",
        "その場合、Attention層とLSTM層は分けずに、同じクラスで書くことになります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwagVW0uGdcR"
      },
      "source": [
        "#### Attentionのマスクについて\n",
        "\n",
        "先程と同様、ミニバッチ化の際には短い系列に対してpaddingを行いますが、Encoderのpadding部分にはAttentionの適用を回避したいわけです。\n",
        "\n",
        "そこで、padding部分は$\\exp\\left[\\mathrm{score}\\left(\\boldsymbol{\\bar{h}}_s,\\ \\boldsymbol{h}_t\\right)\\right]$ が0になるように (score関数の値がとても小さな値になるように) マスクをかけます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi8zHiofGdcR"
      },
      "source": [
        "#### Attentionを用いたDecoderの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_d9H9IeGdcR"
      },
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, output_size, embed_size, hidden_size, batch_size, n_layers, drop_prob, padding_idx):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_prob = drop_prob\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        self.embed = nn.Embedding(output_size, embed_size, padding_idx=padding_idx)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.fc_hidden = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.fc_encoder = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(batch_size, hidden_size))\n",
        "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, n_layers)\n",
        "        # LSTMの隠れ状態とAttention層で計算されるコンテキストベクトルを\n",
        "        # 連結して入力するためhidden_size+hidden_sizeとしている。\n",
        "        self.fc_out = nn.Linear(hidden_size + hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_output, mask):\n",
        "        batch_size = input.shape[0]\n",
        "        # Decoderへの入力をEmbedding\n",
        "        embedded = # WRITE ME\n",
        "        \n",
        "        # dropoutを適用\n",
        "        # embedded shape: [1, batch_size, encoder_embed_dim]\n",
        "        embedded = # WRITE ME\n",
        "\n",
        "        # Alignment Scoreを計算\n",
        "        # scores shape: [batch_size, hidden_size, 1]\n",
        "        scores = self.fc_hidden(hidden[0]).unsqueeze(2)\n",
        "        # encoder_outputの順番を入れ替えてdot積を計算する: \n",
        "        # [sentence_length, batch_size, hidden_size] \n",
        "        # -> [batch_size, sentence_length, hidden_size]\n",
        "        alignment_scores = torch.bmm(encoder_output.permute(1, 0, 2), scores)\n",
        "\n",
        "        # Alignment ScoreにSoftmax関数を適用してAttentionの重みを取得する\n",
        "        # masked_alignment_scores shape: [batch_size, sentence_length]\n",
        "        masked_alignment_scores = alignment_scores.squeeze(2)*mask\n",
        "\n",
        "        # attn_weights shape: [batch_size, sentence_length]\n",
        "        attn_weights = F.softmax(masked_alignment_scores, dim=1)\n",
        "\n",
        "        # Encoderの出力結果にAttentionの重みを適用しContext Vectorを取得\n",
        "        # context_vector shape: [bathc_size, hidden_dim, 1]\n",
        "        context_vector = torch.bmm(encoder_output.permute(1, 2, 0), attn_weights.unsqueeze(2))\n",
        "        \n",
        "        # LSTMにembeddedを入力し、隠れ状態を取得\n",
        "        _, (hidden, cell) = # WRITE ME\n",
        "        # LSTMの隠れ状態とContext Vectorを結合\n",
        "        # hidden: [1, batch_size, hidden_size]\n",
        "        # context_vector: [batch_size, hidden_size, 1]\n",
        "        output = torch.cat((hidden.squeeze(0), context_vector.squeeze(2)), dim=1)\n",
        "        \n",
        "        # prediction shape: # [batch_size, output_size]\n",
        "        prediction = self.fc_out(output.squeeze(0)) \n",
        "\n",
        "        return prediction, hidden, cell, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvIFTJrvGdcR"
      },
      "source": [
        "# 定義したEncoderとDecoderを用いてSeq2Seqモデルを定義する\n",
        "class Seq2SeqAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        src_text = src[0]  # Sourceのテキストを取得\n",
        "        src_length = src[1]  # Sourceのテキスト長を取得\n",
        "        trg_text = trg[0]  # Targetのテキストを取得\n",
        "        trg_length = trg[1]  # Targetのテキスト長を取得\n",
        "        batch_size = trg_text.shape[1]  # Batch Sizeを取得\n",
        "        max_trg_len = trg_text.shape[0]  # Targetの最大文字列長を取得\n",
        "        trg_vocab_size = self.decoder.output_size  # TargetのVocaburay数を取得\n",
        "\n",
        "        # Encoderの最後の隠れ状態をDecoderの初期値とする\n",
        "        encoder_output, encoder_hidden, encoder_cell, sent_len = self.encoder(src_text, src_length)\n",
        "        \n",
        "        # encoderのpadding文字列に対応するためのmask\n",
        "        mask = torch.zeros(batch_size, max(sent_len)).to(DEVICE)\n",
        "        for i in range(batch_size):\n",
        "            pad_idx = sent_len[i]\n",
        "            mask[i, :pad_idx] = 1\n",
        "\n",
        "        # Decoderの出力結果を保持する配列\n",
        "        outputs = torch.zeros(max_trg_len, batch_size, trg_vocab_size).to(DEVICE)\n",
        "        max_src_len = encoder_output.shape[0]\n",
        "        attentions = torch.zeros(max_trg_len, batch_size, max_src_len).to(DEVICE)\n",
        "\n",
        "        # 文章の始まりを表す<sos>トークンをDecoderに入力する\n",
        "        input = trg_text[0, :]\n",
        "\n",
        "        # 各ステップを実行\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "        for step in range(1, max_trg_len):\n",
        "            # Attentionを実装したDecoderに入力する\n",
        "            output, decoder_hidden, decoder_cell, decoder_attention = \\\n",
        "                self.decoder(input, decoder_hidden, decoder_cell, encoder_output, mask)\n",
        "            outputs[step] = output\n",
        "            attentions[step] = decoder_attention\n",
        "\n",
        "            # 予測結果の配列から、もっとも確率が高い予測トークンを取得する\n",
        "            top1 = # WRITE ME\n",
        "            \n",
        "            # 現在のステップでteacher forcingを用いるかどうか\n",
        "            teacher_force = # WRITE ME\n",
        "\n",
        "            # techer forceを用いる場合はtarget配列から該当する値を抽出し、\n",
        "            # 用いない場合は予測結果を次のステップでの入力とする\n",
        "            # WRITE ME\n",
        "\n",
        "        return outputs, attentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI9jWM-gGdcR"
      },
      "source": [
        "### 2. 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BB3Sl-wGdcR"
      },
      "source": [
        "#### 学習用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I98E6h_NGdcS"
      },
      "source": [
        "def train_attention_model(model, loss_function, optimizer, num_epochs=5, clip=1.0):\n",
        "    # 学習モードに設定\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        for batch in tqdm(train_iter):\n",
        "            optimizer.zero_grad()  # 勾配の初期化\n",
        "            output, _ = model(batch.source, batch.target)  # 予測の計算（順伝播）\n",
        "            \n",
        "            # 予測結果の整形と正解データの抽出\n",
        "            output_size = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_size)\n",
        "            trg_text = batch.target[0]\n",
        "            trg = trg_text[1:].view(-1)\n",
        "            \n",
        "            loss = loss_function(output, trg)  # 損失関数の計算\n",
        "            train_loss += loss.item()  # 損失の加算\n",
        "            loss.backward()  # 勾配の計算(逆伝播)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Gradient Clipping\n",
        "            optimizer.step()  # パラメータの更新\n",
        "            \n",
        "        avg_train_loss = train_loss / len(train)  # 平均損失の計算\n",
        "        print(('Epoch [{}/{}]: train_loss: {train_loss:.5f}')\n",
        "          .format(epoch+1, num_epochs, train_loss=avg_train_loss))\n",
        "        \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F22oLqv7GdcS"
      },
      "source": [
        "#### 評価用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDafY2uUGdcS"
      },
      "source": [
        "def evaluate_attention_model(model, loss_function):\n",
        "    model.eval()  # 推論モードに切替\n",
        "    with torch.no_grad():  # 計算グラフの構築をしないよう設定\n",
        "        test_loss = 0\n",
        "        for batch in tqdm(val_iter):\n",
        "            output, _ = model(batch.source, batch.target, teacher_forcing_ratio=0.0)  # 予測の計算（順伝播）\n",
        "\n",
        "            # 予測結果の整形と正解データの抽出\n",
        "            output_size = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_size)\n",
        "            trg_text = batch.target[0]\n",
        "            trg = trg_text[1:].view(-1)\n",
        "\n",
        "            loss = loss_function(output, trg)  # 損失関数の計算\n",
        "            test_loss += loss.item()\n",
        "\n",
        "        avg_test_loss = test_loss / len(val)\n",
        "        print('Test Loss: {test_loss:.5f}'.format(test_loss=avg_test_loss))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHy7sfbqGdcS"
      },
      "source": [
        "#### モデルの学習と評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H2GzDuMGdcS"
      },
      "source": [
        "input_size = len(SOURCE.vocab)\n",
        "output_size = len(TARGET.vocab)\n",
        "encoder_embed_size = 256\n",
        "decoder_embed_size = 256\n",
        "hidden_size = 512\n",
        "n_layers = 1\n",
        "encoder_dropout = 0.5\n",
        "decoder_dropout = 0.5\n",
        "src_padding_idx = SOURCE.vocab.stoi[\"<pad>\"]\n",
        "trg_padding_idx = TARGET.vocab.stoi[\"<pad>\"]\n",
        "\n",
        "# EncoderクラスとDecoderクラスを生成\n",
        "encoder = Encoder(# WRITE ME)\n",
        "attention_decoder = AttentionDecoder(# WRITE ME)\n",
        "\n",
        "# デバイスを指定して翻訳モデルを生成\n",
        "attention_model =# WRITE ME\n",
        "optimizer =  # WRITE ME\n",
        "loss_function =  # WRITE ME\n",
        "\n",
        "# モデルのパラメータを初期化\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "attention_model.apply(init_weights)\n",
        "\n",
        "# モデルの学習（train_attention_model関数を利用）\n",
        "attention_model =  # WRITE ME\n",
        "\n",
        "# モデルの評価（evaluate_attention_model関数を利用）\n",
        "# WRITE ME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o87jkSVUGdcS"
      },
      "source": [
        "### 3. 翻訳文の生成とattention weightの可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8TcuoqgGdcS"
      },
      "source": [
        "### Attention weightの可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3--hxJILGdcS"
      },
      "source": [
        "`matplotlib`で`Attention weight`を可視化してみましょう。\n",
        "\n",
        "具体的には、訳文の生成を行う際に、原文のどの単語をどれだけ重視したかをヒートマップで可視化します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD3SpI_QGdcT"
      },
      "source": [
        "アテンションで獲得した重み$a = [\\boldsymbol{a}_1(s)\\ \\ \\boldsymbol{a}_2(s)\\ \\ \\cdots\\ \\ \\boldsymbol{a}_T(s)]$を可視化してみます。\n",
        "\n",
        "縦軸に訳文 (日本語)、横軸に原文 (英語)を表示し、重みをヒートマップで表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkzY3buiGdcT"
      },
      "source": [
        "def plot_attention_weight(attention, source_text, predict_text):\n",
        "    # Attentionのヒートマップを作成\n",
        "    atten_matrix = attention.cpu().detach().numpy()  # [target_length, sentence_length]\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax = fig.add_subplot()\n",
        "    cax = ax.matshow(atten_matrix)\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # 軸の設定\n",
        "    ax.set_xticklabels([\"\"]+source_text.split(\" \"), rotation=90)\n",
        "    ax.set_yticklabels([\"\"]+predict_text.split(\" \"))\n",
        "\n",
        "    # ラベルを設定\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btPbilOZGdcT"
      },
      "source": [
        "# ID変換された各単語のリストを文章に変換する\n",
        "def token2text(tokens, VOCAB):\n",
        "    texts = []\n",
        "    for token in tokens[1:]:  # <sos>を除く\n",
        "        text = VOCAB.vocab.itos[token]\n",
        "        if text != \"<eos>\":\n",
        "            texts.append(text)\n",
        "        else:\n",
        "            break       \n",
        "    return \" \".join(texts), len(texts)\n",
        "\n",
        "# 出力例を確認\n",
        "attention_model.eval()  # 推論モードに切替\n",
        "batch = next(iter(val_iter))  \n",
        "predicts, attentions = attention_model(batch.source, batch.target, teacher_forcing_ratio=0.0)  # 予測の計算\n",
        "predicts = predicts.argmax(2)  # 予測値のみを取得\n",
        "for i in range(5):\n",
        "    source = batch.source[0][:, i]\n",
        "    target = batch.target[0][:, i]\n",
        "    source_text, source_len = token2text(source, SOURCE)\n",
        "    target_text, target_len = token2text(target, TARGET)\n",
        "    predict_text, predict_len = token2text(predicts[:, i], TARGET)\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    print(\"English (input): {}\".format(source_text))\n",
        "    print(\"Japanese (answer): {}\".format(target_text))\n",
        "    print(\"predicted label: {}\\n\".format(predict_text))\n",
        "\n",
        "    # plot_attention_weight関数を用いてAttentionの重みを確認\n",
        "    # attentions: [max_predict_length, batch_size, max_source_length]\n",
        "    # WRITE ME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmE0-xFSGdcT"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVLDuhCJGdcT"
      },
      "source": [
        "### 【発展】 自然言語処理における最新の手法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKW5pcdfGdcT"
      },
      "source": [
        "最後に自然言語処理の分野で近年注目が集まってきている`BERT (Bidirectional Encoder Representations from Transformers)`と`GPT-3`について紹介します。\n",
        "\n",
        "`BERT`と`GPT-3`ともに、`Transformer`と呼ばれるモデルがベースとなっています。\n",
        "\n",
        "そのため、まずはTransformerについて紹介します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24Zy7uxGdcT"
      },
      "source": [
        "## ・Transformerについて\n",
        "\n",
        "`Transformer`は、先程紹介したSequence Modelの一つですが、RNNやCNNを利用せずAttentionのみを用いたモデルである点が特徴的です。そのため、RNNのように逐次的にモデルを学習する必要がなく並列化が容易なため計算時間を大きく削減できるといったメリットもあります。\n",
        "\n",
        "Transformerは以下の図のような構造をしており、左側がEncoderで右側がDecoderとなっています。論文中では、EncoderとDecoderともにN=6層となっており、Decoder側にはEncoderの出力を受け取る`Multi-Head Attention`が中間に入っています。Decoder側の一つ目(一番下)のAttentionは、`Masked Multi-Head Attention`となっており、その他のAttentionと異なります。これは、Decoderに未来の情報が漏れないようにするためにMask処理を行ったもので、その他の点については`Multi-Head Attention`と同じ構造をしています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpKBogxhGdcT"
      },
      "source": [
        "<img src=\"./figures/transformer.png\" width=\"1000mm\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g0U8NCdGdcT"
      },
      "source": [
        "Transformerの構造で特に重要な要素は、`Scaled Dot-Product Attention`、`Multi-Head Attention`と`Positional Encoding`の3つになります。\n",
        "\n",
        "<img src=\"./figures/transformer_attention.png\" width=\"550mm\">\n",
        "\n",
        "\n",
        "- **Scaled Dot-Product Attention**\n",
        "\n",
        "  - 計算式は以下で与えられており、$ \\sqrt{d_{k}} $で表されるスケール項が存在する以外は、課題3で紹介したものと大きく変わりません。式中の$QK^{T}$をスコア関数と捉えて解釈してください。\n",
        "  - $ \\sqrt{d_{k}} $がスケーリング項として作用します。$d_{k}$の値が大きい時は、Softmax関数の勾配がとても小さくなり、Dot-Product Attentionがうまく機能しないと考えられており、その調整のためにスケーリングを行っています。\n",
        "  - 図中のMaskは、Decoderに未来の情報がリークしないように機能します。\n",
        "  \n",
        "$$ Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V $$\n",
        "\n",
        "\n",
        "- **Multi-Head Attention**\n",
        "\n",
        "  - Multi-Head AttentionとはScaled Dot-Product Attentionを一つのHeadとして、複数のHeadを並列化したAttentionとなっています。\n",
        "  - 複数のHeadからの出力を結合し、線形層に入力することで最終的なAttentionの結果を計算します。\n",
        "\n",
        "\n",
        "- **Positional Encoding**\n",
        "\n",
        "  - Transformerは、再帰 (recurrent)や畳み込み (convolution)を利用していないため、文章中の単語の位置 (順序)に関する情報を保持していません。そのため、`Positional Encoding`の行列 (PE)を入力に加算することで単語の位置情報を考慮できるようにしています。\n",
        "  - PEは単語の位置によって一意な値を取るようになっています。論文中では、PEをsin波とcos波を用いて以下のように表現しています。`pos`は単語の位置を`i`は成分成分次元を表します。\n",
        "\n",
        "\n",
        "$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) $$\n",
        "$$ PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}}) $$\n",
        "\n",
        "\n",
        "参考文献： Attention Is All You Need (https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfMsHSx4GdcU"
      },
      "source": [
        "### モデルの定義\n",
        "\n",
        "PyTorchでは、`torch.nn.Transformer`でTransformerを利用することができます。では、実際に先程の翻訳データセットに対して`Transformer`を活用してみます。\n",
        "\n",
        "torch.nn.Transformerは、以下の9つの引数を取ります。\n",
        "- **d_model**: encoder, decoderへ入力される特徴量の次元 (default=512)\n",
        "- **nhead**: multi-head Attentionで利用するHead数 (default=8)\n",
        "- **num_encoder_layers**: encoderの数 (default=6)\n",
        "- **num_decoder_layers**: decoderの数 (default=6)\n",
        "- **dim_feedforward**: feed foward networkの次元 (default=2048)\n",
        "- **dropout**: ドロップアウト率 (default=0.1)\n",
        "- **activation**: 活性化関数 (default=relu)\n",
        "- **custom_encoder**: 自分で作成したencoder (defalut=None)\n",
        "- **custom_decoder**: 自分で作成したdecoder (defalut=None)\n",
        "\n",
        "\n",
        "公式ドキュメントには、サンプルコードなども記載されているため詳しく知りたい方は確認してみてください。\n",
        "\n",
        "`torch.nn.Transfomer`のドキュメント: https://pytorch.org/docs/master/generated/torch.nn.Transformer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPLm74CkGdcU"
      },
      "source": [
        "# Positional Encodingの項で説明した数式を実装し、入力系列に足し合わせるクラス\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Positional Encodingの数式にしたがって実装\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe) # peは最適化しないパラメータ\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Positional Encodingの情報を足し合わせる\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "    \n",
        "\n",
        "# 翻訳を行うTransformerを実装したクラス\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_input_size, trg_input_size, \n",
        "                 embed_size, src_padding_idx, trg_padding_idx):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.src_padding_idx = src_padding_idx\n",
        "        self.trg_padding_idx = trg_padding_idx\n",
        "        self.output_size = trg_input_size\n",
        "        \n",
        "        # embedding層を定義\n",
        "        self.src_embed = nn.Embedding(src_input_size, embed_size, padding_idx=src_padding_idx)\n",
        "        self.trg_embed = nn.Embedding(trg_input_size, embed_size, padding_idx=trg_padding_idx)\n",
        "        \n",
        "        # positional encoding層を定義\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, max_len=src_input_size)\n",
        "        \n",
        "        # 今回は、レイヤー数などのパラメータを論文と同じ値に設定します\n",
        "        self.transformer = nn.Transformer(\n",
        "                                d_model=embed_size,\n",
        "                                nhead=8, \n",
        "                                num_encoder_layers=6,\n",
        "                                num_decoder_layers=6)\n",
        "        self.linear = nn.Linear(embed_size, self.output_size)\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        # sourceとtraget系列を取得\n",
        "        # output shape: (sequence_length, batch_size)\n",
        "        src_text = src[0] \n",
        "        trg_text = trg[0]\n",
        "        \n",
        "        # 各系列をEmbeddingし、positional encodingを行う\n",
        "        # output shape: (sequence_length, batch_size, embed_size)\n",
        "        src_embedded = self.src_embed(src_text)\n",
        "        src_embedded = self.pos_encoder(src_embedded)\n",
        "        trg_embedded = self.trg_embed(trg_text)\n",
        "        trg_embedded = self.pos_encoder(trg_embedded)\n",
        "        \n",
        "        # 未来の系列がリークすることを防ぐマスク\n",
        "        # マスクを生成するメソッドは、デフォルトでnn.Transformerに用意されています\n",
        "        # output shape: (sequence_length, batch_size, embed_size)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_text.shape[0]).to(DEVICE)\n",
        "        \n",
        "        # Transformerに入力し、出力結果を線形層に入力する\n",
        "        # output shape: (sequence_length, batch_size, output_size)\n",
        "        output = self.transformer(src_embedded, trg_embedded, tgt_mask=trg_mask)\n",
        "        output = self.linear(output)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDafi0WTGdcU"
      },
      "source": [
        "### 学習\n",
        "\n",
        "モデルの学習と評価には、Seq2Seqモデルで利用した`train_seq2seq_model`と`evaluate_seq2seq_model`を用います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMPNt_JeGdcU"
      },
      "source": [
        "# ハイパーパラメータの設定\n",
        "src_input_size = len(SOURCE.vocab)\n",
        "trg_input_size = len(TARGET.vocab)\n",
        "embed_size = 512\n",
        "src_padding_idx = SOURCE.vocab.stoi[\"<pad>\"]\n",
        "trg_padding_idx = TARGET.vocab.stoi[\"<pad>\"]\n",
        "\n",
        "# Transformerを定義\n",
        "transformer_model = Transformer(\n",
        "    src_input_size, \n",
        "    trg_input_size, \n",
        "    embed_size, \n",
        "    src_padding_idx, \n",
        "    trg_padding_idx).to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(transformer_model.parameters(), lr=0.0003)\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=trg_padding_idx)\n",
        "\n",
        "# モデルのパラメータを初期化   \n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.01, 0.01)\n",
        "transformer_model.apply(init_weights)\n",
        "\n",
        "# モデルの学習（train_seq2seq_model関数を再利用）\n",
        "# 1epochに、1分程度時間がかかります。\n",
        "# num_epochs=20で比較的納得感のある翻訳モデルを作成することができますが、\n",
        "# 動作確認で十分の場合は、epoch数を減らしてみてください。\n",
        "transformer_model = train_seq2seq_model(\n",
        "    transformer_model, loss_function, optimizer, num_epochs=20)\n",
        "\n",
        "# モデルの評価（evaluate_seq2seq_model関数を再利用）\n",
        "evaluate_seq2seq_model(transformer_model, loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvsO-GyrGdcU"
      },
      "source": [
        "### 予測結果の確認\n",
        "\n",
        "トレーニングデータで学習した`Transformer`モデルを用いて、テストデータに対する出力結果を確認してみます。\n",
        "出力結果の確認は、Seq2Seqモデルで利用した`print_seq2seq_result`を利用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLASeixQGdcU"
      },
      "source": [
        "# 出力例を確認（print_seq2seq_result関数を再利用）\n",
        "print_seq2seq_result(transformer_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhwsxh35GdcV"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzWPKlMiGdcV"
      },
      "source": [
        "## ・BERTについて\n",
        "\n",
        "`BERT`とは`Transformer`を複数用いて文脈を双方向に学習した`Encoder`モデルです。\n",
        "\n",
        "ベンチマークとして用いられている自然言語処理のタスク11個において、`SOTA (State of The Art)`を達成し、今までのスコアを大きく更新しました。加えて、事前学習済みの`BERT`を用いてファインチューニングすることで様々なタスクに対して、簡単に適用できるといった点や今までのモデルと比べて汎用性が高い点など実利用での利点もあります。\n",
        "\n",
        "`BERT`の基本的な構造は図のようになっており、`Trm`と書かれた青色の丸が上記で紹介した`Transformer`を表しています。\n",
        "具体的な内容までは今回の講義で扱いませんが、`BERT`は以下の2つのタスクで学習を行っています。\n",
        "\n",
        "#### 1. Masked Language Model\n",
        "`Masked Language Model`は穴埋め問題を解くようなタスクです。例えば、以下のような文章があったときにランダムに`Mask`をかけて前後の文脈から、その`Mask`された単語を予測します。\n",
        "\n",
        "```\n",
        "私は犬を連れて海辺を散歩しました。　→　私は犬を連れて [Mask] を散歩しました。\n",
        "```\n",
        "\n",
        "\n",
        "#### 2. Next Sentence Prediction\n",
        "`Next Sentence Prediction`は2つの文章を入力として与え、その2つの文章に関係性があるかどうかを判断するタスクです。学習時には、入力される2つの文章において後半の文章がランダムに50%の確率で他の文章に置換されます。その文章ペアから前半の文章をもとに、後半の文章が関連性があるかどうかを判定していきます。\n",
        "\n",
        "\n",
        "```\n",
        "入力：　今日はとても天気がよかった。(前半)/ 溜まっていた洗濯物を干した。(後半)\n",
        "出力：関係性あり\n",
        "\n",
        "入力：今日はとても天気がよかった。(前半)/ 仕事が無事に終わった。(後半、置換)\n",
        "出力：関係性なし\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TnXtEmIGdcV"
      },
      "source": [
        "<img src=\"./figures/bert.png\" width=\"300mm\">\n",
        "\n",
        "参考文献： BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (https://arxiv.org/abs/1810.04805)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMO97FOlGdcV"
      },
      "source": [
        "`BERT`の実装については`Tensorflow`を用いたものが`Google`から公式に公開されています。しかし、2020年6月現在では、`PyTorch`版は公開されていません。\n",
        "\n",
        "`PyTorch`を用いた`BERT`では、`HuggingFace`が公開している実装が一般的に用いられています。こちらでは様々な言語で事前学習したモデルが用意されています。日本語で事前学習したモデルとしては、東北大学の乾研究室で作成された以下の四つが用意されています。\n",
        "1. bert-base-japanese\n",
        "2. bert-base-japanese-whole-word-mask\n",
        "3. bert-base-japanese-char\n",
        "4. bert-base-japanese-char-whole-word-masking\n",
        "\n",
        "今回は例としてChapter07で行った`Sentiment Analysis`を日本語で事前学習された`BERT`を用いて行ってみます。\n",
        "\n",
        "まず、`pip`経由で必要なライブラリをインストールします。\n",
        "\n",
        "\n",
        "PyTorch版BERT: https://github.com/huggingface/transformers  \n",
        "各種事前学習モデルについて: https://huggingface.co/transformers/pretrained_models.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvb8pH67GdcV"
      },
      "source": [
        "# 日本語で事前学習したBERTの動作確認\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "print(\"Tokenizerの単語数: {0}\".format(len(tokenizer.vocab)))\n",
        "\n",
        "# 文章をtokenに分割\n",
        "text = \"幅広い領域のデータが集まり、AIの活用範囲が広がった。\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"例: {0}\".format(tokens))\n",
        "\n",
        "# IDに変換\n",
        "indexs = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"ID: {0}\".format(indexs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUH5jCl1GdcV"
      },
      "source": [
        "上記の`text`の値を変更してみて、正しく単語の分割が行われているか確認してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D98rhraGdcV"
      },
      "source": [
        "# WRITE ME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9S5UidFGdcV"
      },
      "source": [
        "### データセットの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plF_gB6zGdcW"
      },
      "source": [
        "まず、Chapter07で用いたSentiment Analysisのデータセットを準備します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbRWImvGGdcY"
      },
      "source": [
        "def bert_tokenizer(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    # 文章の開始を表す<sos>と終了を表す<eos>がsentenceに追加する必要があるので-2をしている\n",
        "    tokens = tokens[:model_max_len-2]\n",
        "    return tokens \n",
        "\n",
        "\n",
        "# 各種特殊文字のindexを取得\n",
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "# モデルで扱える最大文字列長を取得\n",
        "model_max_len = tokenizer.max_model_input_sizes[\"cl-tohoku/bert-base-japanese\"]\n",
        "\n",
        "# データセットを定義\n",
        "TEXT = Field(batch_first=True, sequential=True, include_lengths=True,\n",
        "              use_vocab=False, tokenize=bert_tokenizer, \n",
        "              preprocessing=tokenizer.convert_tokens_to_ids,\n",
        "              init_token=init_token_idx,\n",
        "              eos_token=eos_token_idx,\n",
        "              pad_token=pad_token_idx,\n",
        "              unk_token=unk_token_idx)\n",
        "LABEL = LabelField(dtype = torch.long)\n",
        "\n",
        "\n",
        "# 各種データの読み込み\n",
        "train, val, test = TabularDataset.splits(\n",
        "    path=\"/root/userspace/public/day4/chap09/data/\",\n",
        "    train=\"sentiment_train.csv\", validation=\"sentiment_val.csv\",\n",
        "    test=\"sentiment_test.csv\", format=\"csv\",\n",
        "    fields=[(\"text\", TEXT), (\"label\", LABEL)]\n",
        ")\n",
        "\n",
        "# Labelを数字に変換\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "# イテレータの作成\n",
        "batch_size = 16\n",
        "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "    (train, val, test), batch_size=batch_size, device=DEVICE,\n",
        "    sort=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIJfGrs5GdcY"
      },
      "source": [
        "### モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl94nt2yGdcY"
      },
      "source": [
        "class BERTSentiment(nn.Module):\n",
        "    def __init__(self, bert, output_dim):\n",
        "        super(BERTSentiment, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        embedding_dim = bert.config.to_dict()[\"hidden_size\"]\n",
        "        self.linear1 = nn.Linear(embedding_dim, 256)\n",
        "        self.linear2 = nn.Linear(256, output_dim)\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs = (text, text_length)\n",
        "        inputs_text = inputs[0]  # 入力テキストを取得、 inputs_text = [batch_size, sentence lengths]\n",
        "\n",
        "        # BERTをEncoderとして用いる\n",
        "        # embedded shape: [batch_size, embedding_dim]\n",
        "        _, embedded = self.bert(inputs_text)\n",
        "        \n",
        "        # embeddingの結果をlinear1に入力し、出力結果に活性化関数reluを適用する\n",
        "        # output shape: [batch_size, 128]\n",
        "        output = F.relu(self.linear1(embedded))\n",
        "        \n",
        "        # linear1の結果をlinear2に入力する\n",
        "        # output shape: [batch_size, output_dim]\n",
        "        output = self.linear2(output)\n",
        "        \n",
        "        # 出力をSigmoid関数で変換する\n",
        "        # output shape: [batch_size]\n",
        "        output = self.act(output.squeeze(1))\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtNuBKxhGdcZ"
      },
      "source": [
        "### 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi1ahxvoGdcZ"
      },
      "source": [
        "モデルの学習と評価には、今までと同様に`train_model`と`evaluation`を用います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URMgJOLRGdcZ"
      },
      "source": [
        "# パラメータの設定\n",
        "bert = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
        "output_dim = 1\n",
        "\n",
        "# モデル定義\n",
        "bert_model = BERTSentiment(bert, output_dim).to(DEVICE)\n",
        "loss_function = nn.BCELoss().to(DEVICE)\n",
        "optimizer = optim.Adam(bert_model.parameters(), lr=3e-5)\n",
        "\n",
        "# モデルの学習\n",
        "bert_model = train_model(bert_model, loss_function, optimizer, num_epochs=3)\n",
        "\n",
        "# モデルの評価\n",
        "evaluation(bert_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzJjKSmWGdcZ"
      },
      "source": [
        "### 予測結果の確認"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plx7nhyxGdcZ"
      },
      "source": [
        "トレーニングデータで学習した`BERT`モデルを用いて、テストデータに対する出力結果を確認してみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5ibGnezGdcZ"
      },
      "source": [
        "def token2text(tokens):\n",
        "    texts = \"\"\n",
        "    for token in tokens:\n",
        "        if token != tokenizer.sep_token_id:\n",
        "            texts += \" {}\".format(tokenizer.convert_ids_to_tokens(token.item()))\n",
        "        else:\n",
        "            break\n",
        "    return texts\n",
        "\n",
        "# 出力例を確認\n",
        "bert_model.eval()  # 推論モードに切替\n",
        "batch = next(iter(val_iter))  \n",
        "predicts = bert_model(batch.text)  # 予測の計算\n",
        "predicts = torch.round(predicts)  # 予測値をラベルに変換\n",
        "for i in range(5):\n",
        "    tokens = batch.text[0][i, :]\n",
        "    tokens = tokens[1:]  # 開始を表す記号を除く\n",
        "    print(\"input text: {}\".format(token2text(tokens)))\n",
        "    print(\"answer label: {}\".format(LABEL.vocab.itos[batch.label[i]]))\n",
        "    print(\"predicted label: {}\\n\".format(LABEL.vocab.itos[int(predicts[i])]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaxQ1kbSGdcZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMNBVuX_GdcZ"
      },
      "source": [
        "## GPT-3について"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv8PBvKYGdcZ"
      },
      "source": [
        "`GPT-3`は、人工知能に関する研究を行う非営利団体である[OpenAI](https://openai.com/)から2020年に発表された言語生成モデルです。\n",
        "\n",
        "`GPT-3`の内部構造は、先ほど紹介した`BERT`と同様に`Transfomer`をベースに構築されています。しかし、ニューラルネットワークのパラメータ数と学習データの量が大きく増加しています。\n",
        "\n",
        "`GPT-3`のパラメータ数は1750億個となっており、`BERT`の3.4億と比べかなり大きなモデルになっていることがわかります (`GPT-3`の前バージョンである`GPT-2`では、パラメータ数が15億個でした)。また、事前学習に利用したデータ量に関しても`GPT-3`が45TBで、`BERT`が16GBと約2800倍となっています。パラメータ数とモデルの精度の関係について、理論的な裏付けがあるわけではありませんが、論文中ではパラメータ数を変更しモデルの規模を調整した場合の比較も行われています。結果としては、1750億個のパラメータを使用した、もっとも大きなモデルが高い精度を記録しました。\n",
        "\n",
        "`GPT-3`は、論文のタイトルにあるように`Few-Shot Learning`でモデルの学習を行っています。`Few-Shot Learning`の説明を行うために、`Zero-Shot Learning`と`One-Shot Learning`の説明を行います。詳しい説明については、論文中にも記載があるので興味がある方は確認してみてください。\n",
        "\n",
        "- **Zero-Shot Learning**\n",
        "\n",
        "  - どのようなことを行うのかを表すタスクの説明とクエリのみを入力します。\n",
        "  - 図の場合だと、`\"Translate English to French:\\n cheese => \"`が入力となります。\n",
        " \n",
        "<img src=\"./figures/zero_shot.png\" width=\"550mm\">\n",
        "\n",
        "\n",
        "- **One-Shot Learning**\n",
        "\n",
        "  - どのようなことを行うのかを表すタスクの説明と一つの例、そしてクエリを入力します。\n",
        "  - 図の場合だと、`\"Translate English to French:\\n sea otter => loutre de mer \\n cheese =>\"`が入力となります。\n",
        "  \n",
        "<img src=\"./figures/one_shot.png\" width=\"550mm\">\n",
        "\n",
        "\n",
        "- **Few-Shot Learning**\n",
        "\n",
        "  - どのようなことを行うのかを表すタスクの説明といくつかの例、そしてクエリを入力します。\n",
        "  - 図の場合だと、`\"Translate English to French:\\n sea otter => loutre de mer \\n peppermint => menthe poivree \\n plush girafe => girafe peluche \\n cheese =>\"`が入力となります。\n",
        "  \n",
        "<img src=\"./figures/few_shot.png\" width=\"550mm\">\n",
        "\n",
        "\n",
        "\n",
        "#### 活用例\n",
        "\n",
        "`GPT-3`はプライベートなベータ版のAPIが[公式サイト](https://beta.openai.com/?app=productivity)から公開されていますが、2020年12月現在では順番待ちのリストに申請する必要があります。実際に申請が通過した研究者などが、自然言語を入力としWebサイトを構築するプログラムを生成した場合の利用結果などを公開しているので確認してみると面白いと思います。また、[公式サイト](https://beta.openai.com/?app=productivity)でもいくつかサンプルが公開されています。\n",
        "\n",
        "\n",
        "参考文献： Language Models are Few-Shot Learners (https://arxiv.org/abs/2005.14165)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S79IyhpsGdca"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}